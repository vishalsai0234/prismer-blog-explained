{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccd2c3d",
   "metadata": {},
   "source": [
    "# Prismer: A Vision-Language Model with Multi-Task Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743645b",
   "metadata": {},
   "source": [
    "*Vishal*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11787c",
   "metadata": {},
   "source": [
    "## üß† Motivation\n",
    "\n",
    "As deep learning systems scale, so does the need for efficient and effective multimodal reasoning. The Prismer model offers a promising direction: instead of training huge monolithic architectures on massive datasets, it integrates pre-trained task-specific vision experts into a unified vision-language model, offering both performance and efficiency. I found this paradigm shift interesting and relevant to current trends in efficient deep learning and compositionality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acd836",
   "metadata": {},
   "source": [
    "## üîç Connection with Past & Current Work in Multimodal Learning\n",
    "\n",
    "Historically, vision-language models (VLMs) like UNITER, ViLT, and BLIP rely on large-scale image-text pairs and massive compute. Prismer challenges this by leveraging frozen, pre-trained experts across multiple vision tasks (e.g., depth estimation, OCR, segmentation) and combines them using a lightweight encoder-decoder transformer. It draws inspiration from works like Perceiver, Flamingo, and the concept of Socratic models, while distinguishing itself with strong fine-tuned performance and low compute requirements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6946f7d",
   "metadata": {},
   "source": [
    "## üìö Key Learnings\n",
    "\n",
    "- Multi-task vision signals improve generalization without increasing trainable parameters significantly.\n",
    "- Robustness to noisy experts implies resiliency and flexible deployment.\n",
    "- Fine-tuning only ~20% of the model achieves performance comparable to full fine-tuning.\n",
    "- Leveraging diverse visual experts enriches semantic understanding in downstream tasks like VQA and captioning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99d242",
   "metadata": {},
   "source": [
    "## üß™ Code / Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580e7a5",
   "metadata": {},
   "source": [
    "[Click here](https://github.com/vishalsai0234/prismer-blog-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be6f30",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0621c52",
   "metadata": {},
   "source": [
    "## üí≠ Reflections\n",
    "\n",
    "What surprised me?\n",
    "\n",
    "- Prismer with frozen experts outperforms many fully trainable VLMs with 10‚Äì100√ó more data.\n",
    "\n",
    "- The model‚Äôs resilience to poor-quality (even noisy) experts is counterintuitive and inspiring.\n",
    "\n",
    "Scope for improvement:\n",
    "\n",
    "- Extend Prismer to work with dynamic or missing expert inputs.\n",
    "\n",
    "- Explore sequential reasoning with text-based expert representations (e.g., using Pix2Seq).\n",
    "\n",
    "- Improve adaptability to new experts without full retraining.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23a71d",
   "metadata": {},
   "source": [
    "## üîó References\n",
    "[Prismer GitHub Repo](https://github.com/NVlabs/prismer)\n",
    "\n",
    "[Prismer Paper (2024)](https://arxiv.org/abs/2303.02506)\n",
    "\n",
    "[Visual Haystacks Blog Example](https://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5557c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
